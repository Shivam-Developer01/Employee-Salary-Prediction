# -*- coding: utf-8 -*-
"""EmployeeSalaryPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DL1LG7_Hxtmz1zoSGaMTby_znYLRv5es
"""

import pandas as pd

data=pd.read_csv(r"/content/adult 3.csv")

data

print(data['gender'].value_counts())

# finding null value
data.isnull()

#null values
data.isna().sum() #mean mdeian mode arbitrary

print(data.workclass.value_counts())

data.workclass.replace({'?':'Others'},inplace=True)
print(data['workclass'].value_counts())

print(data['occupation'].value_counts())

data.occupation.replace({'?':'Others'},inplace=True)
print(data['occupation'].value_counts())

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']
print(data['workclass'].value_counts())

print(data.relationship.value_counts())

print(data.gender.value_counts())

data.shape

#outlier detection
import matplotlib.pyplot as plt   #visualization
plt.boxplot(data['age'])
plt.show()

data=data[(data['age']<=75)&(data['age']>=17)]

plt.boxplot(data['age'])
plt.show()

data.shape

data=data.drop(columns=['educational-num']) #redundant features removal

data=data.drop(columns=['fnlwgt'])
data=data.drop(columns=['relationship'])
data=data.drop(columns=['race'])
data=data.drop(columns=['capital-gain'])
data=data.drop(columns=['capital-loss'])
data=data.drop(columns=['native-country'])

data

x=data.drop(columns=['income'])
y=data['income']
x

y

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import joblib


categorical_cols = ['education', 'occupation', 'workclass',  'gender']
numeric_cols = ['age', 'hours-per-week']


# Preprocessing pipeline
preprocessor = ColumnTransformer(transformers=[
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),
    ('num', StandardScaler(), numeric_cols)
])


# Train-test split using the original DataFrame before scaling
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Define Models
models = {
    "LogisticRegession": LogisticRegression(max_iter=1000), # Increased max_iter for convergence
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

results = {}
best_model = None
best_score = 0


# Train and evaluate models
for name, model in models.items():
    pipe = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', model)
    ])
    pipe.fit(x_train, y_train)
    preds = pipe.predict(x_test)
    acc = accuracy_score(y_test, preds)
    results[name] = acc
    print(classification_report(y_test, preds))
    print(f"{name}: {acc:.4f}")
    if acc > best_score:
        best_score = acc
        best_model = pipe

#Save the best model
joblib.dump(best_model, "best_model.pkl")
print("Saved best model as best_model.pkl")



# from sklearn.metrics import classification_report

import matplotlib.pyplot as plt
plt.bar(results.keys(), results.values(), color='green')
plt.ylabel('Accuracy Score')
plt.title('Model Comparison')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# # app.py
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# 
# # Load the trained model
# model = joblib.load("best_model.pkl")
# 
# st.set_page_config(page_title="Employee Salary Classification", layout="centered")
# 
# st.title(" Employee Salary Classification App")
# st.markdown("Predict whether an employee earns > 50k or <= 50k based on input features.")
# 
# # Sidebar inputs (these must match your training features columns)
# st.sidebar.header("Input Employee Details")
# 
# # Replace these fields with your dataset's actual input columns
# age = st.sidebar.slider("Age", 18, 65, 30)
# education = st.sidebar.selectbox("Education Level", [
#     "Bachelors", "Masters", "PhD", "HS-grad", "Assoc", "Some-collage"
# ])
# occupation = st.sidebar.selectbox("Job Role", [
#     "Tech-support", "Craft-repair", "Other-service", "Sales",
#     "Exec-managerial", "Prof-specialty", "Handlers-cleaners", "Machine-op-inspct",
#     "Adm-clerical", "Farming-fishing", "Transport-moving", "Priv-house-serv",
#     "Protective-serv", "Armed-Forces"
# ])
# workclass = st.sidebar.selectbox("Workclass", [
#     "Private", "Self-emp-not-inc", "Local-gov", "Others", "State-gov", "Self-emp-inc", "Federal-gov"
# ])
# 
# gender = st.sidebar.selectbox("Gender", [
#     "Male", "Female"
# ])
# 
# hours_per_week = st.sidebar.slider("Hours per week", 1, 80, 40)
# # experience = st.sidebar.slider("Years of Experience", 0, 40, 5)
# 
# # Build input Dataframe (must match preprocessing of your training data)
# input_df = pd.DataFrame({
#     'age': [age],
#     'education': [education],
#     'occupation': [occupation],
#     'workclass' : [workclass],
#     'gender' : [gender],
#     'hours-per-week': [hours_per_week],
# })
# 
# st.write("### Input Data")
# st.write(input_df)
# 
# # Predict button
# if st.button("Predict Salary Class"):
#     prediction = model.predict(input_df)
#     st.success(f" Prediction: {prediction[0]}")
# 
# # Batch prediction
# st.markdown("---")
# st.markdown("#### Batch Prediction")
# uploaded_file = st.file_uploader("Upload a CSV file for batch prediction", type="csv")
# 
# if uploaded_file is not None:
#     batch_data = pd.read_csv(uploaded_file)
#     st.write("Uploaded data preview:", batch_data.head())
#     batch_preds = model.predict(batch_data)
#     batch_data['PredictedClass'] = batch_preds
#     st.write("Predictions:")
#     st.write(batch_data.head())
#     csv = batch_data.to_csv(index=False).encode('utf-8')
#     st.download_button("Download Predictions CSV", csv, file_name='predicted_classes.csv', mime='text/csv')
#

!pip install streamlit pyngrok

!ngrok authtoken 30EHXk2apUF9wl3u6UpFEsYFkYv_5JMNKERJ8cR41Zd4mC7eK

import os
import threading

def run_streamlit():
  os.system('streamlit run app.py --server.port 8501')

thread = threading.Thread(target=run_streamlit)
thread.start()

from pyngrok import ngrok
import time

# Wait a few seconds to make sure Streamlit started
time.sleep(5)

#Create a tunnel to the Streamlit port 8501
public_url = ngrok.connect(8501)
print("Your Streamlit app is live here:", public_url)